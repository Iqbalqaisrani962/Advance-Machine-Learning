{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4374e488",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "- A gradient (derivative) defines the effects on the output of the function with a little change in the independent variable\n",
    "- In gradient descent, we iteratively changed the weights of the inputs features to find the minimum value of the cost function (differentiable function)\n",
    "- Gradient descent is a first order iterrative algorithm used to find the local minimum of a differentiable function\n",
    "- A function is differentiable, if the derivative of the function exist at every point in its given domain\n",
    "- The process of finding the slope/derivative is called differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e2984",
   "metadata": {},
   "source": [
    "- Maxima and minima (Extrema) of a function are the largest and smallest value of the function respectively, either within a given range or on the entire domain.\n",
    "- Mocal minima: If changes sign from negative to positive, as increases via point , then gives the minimum value of the function in that range.\n",
    "- Local maxima: If changes sign from positive to negative, as increases via point , then gives the maximum value of the function in that range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631f06c2",
   "metadata": {},
   "source": [
    "- Gradient of a function at any point is just a collection of all its partial derivatives with respect to all the dimensions of that function\n",
    "- In case of function of two or more variables we take partial derivatives.\n",
    "- To find the partial derivative of a function with respect to one variable, all the other variables are treated as constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a31a27",
   "metadata": {},
   "source": [
    "- The idea of gradient descent is to take repeated steps in the opposite direction of the gradient of the function at the current point till the minima found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b3f33",
   "metadata": {},
   "source": [
    "#### Challenges of Gradient Descent\n",
    "- Gradient Descent algorithm is not guaranteed to give us the correct solution. \n",
    "- ***There are several circumstances that can make Gradient Descent algorithm either go completely wrong or just a bit wrong:***\n",
    "1) Poor choice of model parameters (starting point, number of iterations, length of each step)\n",
    "2) Stuck in a local minimum due to unfortunate starting point\n",
    "3) Vanishing Gradient\n",
    "4) Exploding Gradient\n",
    "- ***There are various methods that help in overcoming the local minimum and unfortunate starting point:***\n",
    "1) Use stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461df1df",
   "metadata": {},
   "source": [
    "#### Vanishing Gradient ###\n",
    "- ***Vanishing gradient descent is a problem that occurs when the gradient become exponentially small and when multiplied with learning rate the step size becomes so much small that you donot reach the function minimum in specified epochs.***\n",
    "- **In Deep Learning, there are various techniques that help in overcoming the vanishing gradient problems:**\n",
    "1) Instead of sigmoid activation function use activation functions that do not saturate like Rectified\n",
    "2) Linear Unit (ReLU, Leaky-ReLU, Randomized ReLU)\n",
    "3) Long Short Term Memory (LSTM)\n",
    "4) Gated Recurrent Unit (GRU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5321504b",
   "metadata": {},
   "source": [
    "#### Exploding Gradient Problem\n",
    "- ***Exploding gradient is a problem that occurs when the gradient is too steep/large and when multiplied with learning rate the step size becomes so much large that you may miss the minimum and cross the other side***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f7b904",
   "metadata": {},
   "source": [
    "### Linear Regression Using Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891991d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
