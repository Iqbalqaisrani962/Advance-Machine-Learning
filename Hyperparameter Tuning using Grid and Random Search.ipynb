{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0d82e1",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning using Grid and Random Search\n",
    "#### Model Parameters VS Hyperparameters\n",
    "- 1) Hyperparameters are set manually by ML engineer/practitioner prior to the start of the model’s training.\n",
    "- 1) Model parameters are learnt by the learning algorithm during the training phase.\n",
    "- 2) Hyperparameters are used to optimize machine learning model.\n",
    "- 2) Model’s parameters are later used for prediction.\n",
    "- 3) They are internal to the model.\n",
    "- 3) Thy are external to the model\n",
    "- 4) Examples: Value of K in KNN, learning rate for training a neural network, number of trees in RandomForrest.\n",
    "- 4) Examples: Coefficients in a Linear or Logistic regression, support vectors in a support vector machine, and weights in an     artificial neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fce3bb",
   "metadata": {},
   "source": [
    "#### How to find the hyperparameters of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feeffc8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.0,\n",
       " 'copy_X': True,\n",
       " 'fit_intercept': True,\n",
       " 'max_iter': None,\n",
       " 'positive': False,\n",
       " 'random_state': None,\n",
       " 'solver': 'auto',\n",
       " 'tol': 0.0001}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa850f59",
   "metadata": {},
   "source": [
    "### How to find best parameters of our model\n",
    "***Hyperparameter Turing Techniques***\n",
    "- 1) ***By Hand***: Select the hyperparameters values based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results.\n",
    "- 2) ***GridSearchCV***: Set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameter values is tried which can be very inefficient.\n",
    "- 3) ***RandomizedSearchCV***: Set up a grid of hyperparameter values and select random combinations to train the model and score. The number of search iterations is based on time/resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3efd3",
   "metadata": {},
   "source": [
    "#### Hypertuning Steps\n",
    "1) Make a list of different hyperparameters based on the problem in hand. If there are more than one hyperparameter then make grid with different combination of parameters\n",
    "2) Fit all of them separately to the model.\n",
    "3) Note down the model performance\n",
    "4) Choose the best performing one\n",
    "5) Always use cross validation technique for hyperparameter tuning to avoid the model overfitting on test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f75b5c",
   "metadata": {},
   "source": [
    "## 3. Find Optimized Hyperparameter By Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c382f85",
   "metadata": {},
   "source": [
    "### a. Find Optimized Hyperparameter by Trial and Error\n",
    "- Approach 1: Use train_test_split and manually tune parameters by trial and error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97e38617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score:  0.769629851355745\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "df = pd.read_csv(\"datasets/advertising4D.csv\")\n",
    "X = df.drop('sales', axis=1)\n",
    "y = df['sales']\n",
    "# Do a train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "# SCALE DATA\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Specify values of hyperparameters\n",
    "model = Ridge(alpha=100, solver='lsqr')\n",
    "# Train\n",
    "model.fit(X_train, y_train)\n",
    "# Evaluate\n",
    "r2 = r2_score(y_test, model.predict(X_test))\n",
    "r2 = model.score(X_test, y_test)\n",
    "print(\"R2 Score: \", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99621c26",
   "metadata": {},
   "source": [
    "- Each time you execute the above code cell, you get different R2 scores. None of these R2 scores is the true representation of the entire dataset. This is because of the train-test split. The limitation is each time the model is tested on different 20% of the test set. The solution is to use some Cross-Validation technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6a7fe",
   "metadata": {},
   "source": [
    "### Use of cross_val_score() Method (KFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a8c0b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 scores for all the folds:  [0.74168497 0.73276148 0.7480924  0.75002622 0.72592539]\n",
      "Mean R2 score:  0.7396980922198394\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "df = pd.read_csv(\"datasets/advertising4D.csv\")\n",
    "X = df.drop('sales', axis=1)\n",
    "y = df['sales']\n",
    "# Do a train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "# SCALE DATA\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#cross_val_score() is a cross validation method that trains and tests a model over mu\n",
    "cv_scores = cross_val_score(\n",
    "              estimator = Ridge(alpha=100, solver='lsqr'), \n",
    "              X = X_train, \n",
    "              y = y_train, \n",
    "              scoring = 'r2',\n",
    "              cv = 5 #Instead of integer value you can pass KFold obect\n",
    " )\n",
    "print(\"R2 scores for all the folds: \", cv_scores)\n",
    "print(\"Mean R2 score: \", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eae291",
   "metadata": {},
   "source": [
    "### b. Find Optimized Hyperparameter using For Loops\n",
    "#### (i) Use Simple Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d294b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 lsqr : 0.2456569245047806\n",
      "1000 svd : 0.2456569245047806\n",
      "100 lsqr : 0.8004208874327983\n",
      "100 svd : 0.8004208874327985\n",
      "10 lsqr : 0.9350626771176971\n",
      "10 svd : 0.9350626771176971\n",
      "5 lsqr : 0.9362068233757561\n",
      "5 svd : 0.9362068233757561\n",
      "1 lsqr : 0.9356287521153018\n",
      "1 svd : 0.9356287521153018\n",
      "0.8 lsqr : 0.935559868216586\n",
      "0.8 svd : 0.935559868216586\n",
      "0.5 lsqr : 0.9354489002095089\n",
      "0.5 svd : 0.9354489002095089\n",
      "0.2 lsqr : 0.9353286478012036\n",
      "0.2 svd : 0.9353286478012036\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/advertising4D.csv\")\n",
    "X = df.drop('sales', axis=1)\n",
    "y = df['sales']\n",
    "# Do a train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "# SCALE DATA\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "alpha_list = [1000, 100, 10, 5, 1, 0.8, 0.5, 0.2]\n",
    "solver_list = ['lsqr', 'svd']\n",
    "for a in alpha_list: \n",
    "     for s in solver_list: \n",
    "          model = Ridge(alpha=a, solver=s)# Instentiate model each time with different \n",
    "          model.fit(X_train, y_train) # Fit the model to training data\n",
    "          r2 = model.score(X_test, y_test) # Evaluate by calculating R2 Score\n",
    "          print(a, s,\":\",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712b17b",
   "metadata": {},
   "source": [
    "***Limitation:***\n",
    "- Model evaluation is done on just 20% of the data.\n",
    "- After finalizing the best combination of hyperparameters, we are not left with any unseen data on\n",
    "- which we can do the final evaluation of the model with the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc4f8c9",
   "metadata": {},
   "source": [
    "#### (ii) Use Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fde70f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 lsqr : 0.18308426373164532\n",
      "1000 svd : 0.18307934795177522\n",
      "100 lsqr : 0.7177160715206126\n",
      "100 svd : 0.7177160715206126\n",
      "10 lsqr : 0.8770035136002179\n",
      "10 svd : 0.8770035136002179\n",
      "5 lsqr : 0.8799663254092133\n",
      "5 svd : 0.8799663254092133\n",
      "1 lsqr : 0.8807775554934073\n",
      "1 svd : 0.8807775554934073\n",
      "0.8 lsqr : 0.8807758504716677\n",
      "0.8 svd : 0.8807758504716677\n",
      "0.5 lsqr : 0.8807651636870044\n",
      "0.5 svd : 0.8807651636870044\n",
      "0.2 lsqr : 0.8807445895239574\n",
      "0.2 svd : 0.8807445895239576\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/advertising4D.csv\")\n",
    "X = df.drop('sales', axis=1)\n",
    "y = df['sales']\n",
    "# Do a train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "# SCALE DATA\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "alpha_list = [1000, 100, 10, 5, 1, 0.8, 0.5, 0.2]\n",
    "solver_list = ['lsqr', 'svd']\n",
    "for a in alpha_list: \n",
    "    for s in solver_list: \n",
    "        cv_scores = cross_val_score(Ridge(alpha=a, solver=s ), X_train, y_train, scoring = 'r2', cv = 5)\n",
    "        print(a, s,\":\",np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7852e74d",
   "metadata": {},
   "source": [
    "## 4. Find Optimized Hyperparameters using GridSearchCV()\n",
    "- Grid search is a method for hyperparameter optimization that involves specifying a list of values for each hyperparameter that you want to optimize, and then training a model for each combination of these values.\n",
    "- Basically, we divide the domain of the hyperparameters into a discrete grid.\n",
    "- Then, we try every combination of values of this grid, calculating some performance metrics using cross\u0002validation.\n",
    "- The point of the grid that maximizes the average value in cross-validation, is the optimal combination of values for the hyperparameters.\n",
    "- Additionally, it is recommended to use cross-validation when performing hyperparameter optimization. This can provide a more accurate estimate of the model’s performance and help to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30aec5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.8810203980478049\n",
      "Best Score:  {'alpha': 0.5, 'solver': 'lsqr'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "df = pd.read_csv(\"datasets/advertising4D.csv\")\n",
    "X = df.drop('sales', axis=1)\n",
    "y = df['sales']\n",
    "# Do a train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "# SCALE DATA\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#Dictionary with parameters names (`str`) as keys and lists of parameter settings to \n",
    "params = { 'alpha': [1000, 100, 10, 1, 0.5],\n",
    " 'solver': ['lsqr', 'svd'] }\n",
    "gs = GridSearchCV(estimator=Ridge(), \n",
    " param_grid=params,\n",
    "scoring='r2',\n",
    "cv=5,\n",
    "n_jobs=-1) \n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Best Score: \", gs.best_score_)\n",
    "print(\"Best Score: \", gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd972eba",
   "metadata": {},
   "source": [
    "***Limitations of GridSearchCV:***\n",
    "- Grid search is an exhaustive algorithm that spans all the combinations, so it can actually find the best point in the domain.\n",
    "- For that it trains a separate model for every combination of hyperparameter values.\n",
    "- Suppose you have million of data points in your dataset and a bundle of hyperparameters and their values to tune. In that case the grid will be multidimensional and the algorithm will become computationally expensive as well as time consuming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37342376",
   "metadata": {},
   "source": [
    "## 5. Find Optimized Hyperparameters using RandomizedSearchCV()\n",
    "- Random search is similar to grid search, but instead of using all the points in the grid, it tests only a randomly selected subset of these points.\n",
    "- The smaller this subset, the faster but less accurate the optimization. The larger this dataset, the more accurate the optimization but the closer to a grid search.\n",
    "- Random search is a very useful option when you have several hyperparameters with a fine\u0002grained grid of values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afb6827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.903727913298369\n",
      "Best Score:  {'solver': 'svd', 'alpha': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "df = pd.read_csv(\"datasets/advertising4D.csv\")\n",
    "X = df.drop('sales', axis=1)\n",
    "y = df['sales']\n",
    "# Do a train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "# SCALE DATA\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#Dictionary with parameters names (`str`) as keys and lists of parameter settings to \n",
    "params = { 'alpha': [1000, 100, 10, 1, 0.5],\n",
    " 'solver': ['lsqr', 'svd'] }\n",
    "rs = RandomizedSearchCV(estimator=Ridge(), \n",
    " param_distributions=params,\n",
    "n_iter=6, #Number of parameter combinations to try.\n",
    " scoring='r2',\n",
    "cv=5,\n",
    "n_jobs=-1) \n",
    "rs.fit(X_train, y_train)\n",
    "print(\"Best Score: \", rs.best_score_)\n",
    "print(\"Best Score: \", rs.best_params_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
